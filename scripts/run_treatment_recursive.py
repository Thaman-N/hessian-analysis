"""
TREATMENT: Recursive Training on SmolLM2-135M
Each generation is trained on synthetic data generated by the previous generation.
With checkpoint/resume functionality.
"""

import os
import torch
import argparse
import subprocess
from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup
from datasets import Dataset, load_from_disk
from torch.utils.data import DataLoader
from tqdm import tqdm
import random
import gc

# --- CONFIG ---
# MODEL_ID = "HuggingFaceTB/SmolLM2-135M"
# GPT2 (117M params - OpenAI's original)
# MODEL_ID = "openai-community/gpt2"

MODEL_ID = "facebook/opt-125m"
MAX_LENGTH = 256
BATCH_SIZE = 8
GEN_BATCH_SIZE = 32  # Larger batches for faster generation
LR = 5e-5
SAMPLES = 50000  # Match Controls A & B (SmolLM experiments)

def run_recursive_treatment(generations):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    for gen in range(1, generations + 1):
        print(f"\n{'='*80}")
        print(f"TREATMENT (OPT Recursive): Generation {gen}")
        print(f"{'='*80}")
        
        # Define paths
        model_dir = f"models/opt_treatment_gen_{gen}"
        result_dir = f"results/opt_treatment_gen_{gen}"
        data_path = f"data/opt_treatment_synthetic_gen_{gen}"
        
        # Check if this generation is already complete
        if os.path.exists(f"{result_dir}/hessian_stats.json"):
            print(f"âœ… Generation {gen} already complete (found Hessian results), skipping...")
            continue
        
        # --- PHASE 1: GENERATE SYNTHETIC DATA ---
        source_model_path = MODEL_ID if gen == 1 else f"models/opt_treatment_gen_{gen-1}"
        
        if os.path.exists(data_path):
            print(f"[PHASE 1] âœ… Synthetic data already exists at {data_path}, skipping generation...")
        else:
            print(f"\n[PHASE 1] Generating data from: {source_model_path}")
            
            gen_model = AutoModelForCausalLM.from_pretrained(
                source_model_path,
                torch_dtype=torch.float16
            ).to(device)
            gen_tok = AutoTokenizer.from_pretrained(source_model_path)
            gen_tok.padding_side = "left"
            if gen_tok.pad_token is None:
                gen_tok.pad_token = gen_tok.eos_token
            
            prompts = [
                "Once upon a time",
                "Create a story about",
                "One day",
                "There was a little",
                "The girl wanted",
                "A boy found",
                "The dog",
                "In the forest"
            ]
            
            stories = []
            gen_model.eval()
            
            print(f"Generating {SAMPLES} synthetic stories...")
            with torch.no_grad():
                pbar = tqdm(total=SAMPLES, desc="Generating")
                while len(stories) < SAMPLES:
                    batch_prompts = [random.choice(prompts) for _ in range(GEN_BATCH_SIZE)]
                    inputs = gen_tok(batch_prompts, return_tensors="pt", padding=True).to(device)
                    
                    outputs = gen_model.generate(
                        **inputs,
                        max_new_tokens=200,
                        do_sample=True,
                        temperature=0.8,
                        top_k=50,
                        pad_token_id=gen_tok.pad_token_id,
                        eos_token_id=gen_tok.eos_token_id
                    )
                    
                    decoded = gen_tok.batch_decode(outputs, skip_special_tokens=True)
                    stories.extend(decoded)
                    pbar.update(len(decoded))
                pbar.close()
            
            Dataset.from_dict({"text": stories[:SAMPLES]}).save_to_disk(data_path)
            print(f"âœ… Saved {SAMPLES} stories to {data_path}")
            
            del gen_model
            torch.cuda.empty_cache()
            gc.collect()
        
        # --- PHASE 2: TRAIN ON SYNTHETIC DATA ---
        if os.path.exists(model_dir) and os.path.exists(f"{model_dir}/config.json"):
            print(f"\n[PHASE 2] âœ… Model already exists at {model_dir}, skipping training...")
        else:
            print(f"\n[PHASE 2] Training Generation {gen}...")
            
            model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(device)
            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
            tokenizer.padding_side = "right"
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                model.config.pad_token_id = tokenizer.eos_token_id
            
            dataset = load_from_disk(data_path)
            
            def tokenize_function(examples):
                return tokenizer(
                    examples["text"],
                    truncation=True,
                    padding="max_length",
                    max_length=MAX_LENGTH,
                    return_tensors="pt"
                )
            
            dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
            dataset.set_format("torch")
            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
            
            optim = torch.optim.AdamW(model.parameters(), lr=LR)
            sched = get_linear_schedule_with_warmup(optim, num_warmup_steps=100, num_training_steps=len(loader))
            
            model.train()
            for batch in tqdm(loader, desc=f"Training Gen {gen}"):
                batch = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**batch, labels=batch["input_ids"])
                
                outputs.loss.backward()
                optim.step()
                sched.step()
                optim.zero_grad()
            
            print(f"\n[PHASE 3] Saving model to {model_dir}...")
            model.save_pretrained(model_dir)
            tokenizer.save_pretrained(model_dir)
            
            del model
            del optim
            del sched
            torch.cuda.empty_cache()
            gc.collect()
        
        # --- PHASE 3: HESSIAN ANALYSIS ---
        if os.path.exists(f"{result_dir}/hessian_stats.json"):
            print(f"\n[PHASE 4] âœ… Hessian results already exist, skipping analysis...")
        else:
            print(f"\n[PHASE 4] Running Hessian analysis...")
            print("ðŸ§¹ Clearing VRAM before Hessian analysis...")
            torch.cuda.empty_cache()
            gc.collect()
            
            subprocess.run([
                "python", "scripts/hessian_analysis_generic.py",
                "--generation", str(gen),
                "--model_path", model_dir,
                "--output_dir", result_dir
            ], check=True)
        
        print(f"\nâœ… Generation {gen} complete!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--generations", type=int, default=5)
    args = parser.parse_args()
    
    print("=" * 80)
    print("TREATMENT: RECURSIVE TRAINING ON OPT-125M (with checkpoint/resume)")
    print("=" * 80)
    
    run_recursive_treatment(args.generations)
    
    print("\n" + "=" * 80)
    print("TREATMENT COMPLETE!")
    print("=" * 80)